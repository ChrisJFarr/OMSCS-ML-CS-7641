Decision tree

## Preprocessing
* no preprocessing was utilized for all of the decision tree experiments. This is because I know that decision trees are
    fairly robust to irregular scales and distributions.
    

validation curve parameters for experiments 1, 2 and 3
max_depth:
    start: 2
    stop: 10
    step: 1
ccp_alpha:
    start: 0.0
    stop: 0.051
    step: 0.001
min_samples_split:
    start: 2
    stop: 500
    step: 25
min_samples_leaf:
    start: 1
    stop: 500
    step: 25

#################
Tree Experiment 1:
#################

## Validation Curve
* Starting from default parameters, performed validation curve analysis on the following parameters:
    * Top performance is test AUC=.81
        * achieved with all defaults and setting min_samples_leaf to 26
            * train AUC .879 test AUC=.812
        * achieved with all defaults and setting min_samples_split to 102
            * train AUC .862 test AUC=.810 (slightly less overfitting) 
    * max-depth
        * train .854 and test .793 with max-depth=3
        * this reduced overfitting at cost of some performance compared to top metrics
    * ccp_alpha
        * train .848 and test .794 with ccp_alpha=0.01
        * similar effect of max-depth on test performance (when viewing the curve) but inverse
            impact on the training
Validation curve completed in 34.3 seconds

Additionally I compared the performance of gini vs entropy for the validation curves.
The performance was very similar although in some cases entropy had a test auc about .002
higher than gini. The runtime was also slightly longer, as i expected based on some research
about runtimes comparing the two (https://quantdare.com/decision-trees-gini-vs-entropy/)
I decided to stick with gini to have consistent default parameters throughout the three tree
experiments since the ds2 is significantly larger than ds1 and this could hurt the runtime for
the experiments.

## Grid-Search

grid-search parameters
* Performed grid-search cv from defaults and through max-performing values discovered above


Best parameters
Round 1 of grid-search
{'ccp_alpha': 0.0,
 'max_depth': 5,
 'min_samples_leaf': 16,
 'min_samples_split': 92}
'Best performance: 0.814'
Grid search completed in 284.4 seconds
Round 2 of grid-search
{'ccp_alpha': 0.0,
 'max_depth': 6,
 'min_samples_leaf': 20,
 'min_samples_split': 90}
'Best performance: 0.814'
Grid search completed in 120.1 seconds
Round 3 of grid-search
Best parameters
{'ccp_alpha': 0.0004,
 'max_depth': 6,
 'min_samples_leaf': 17,
 'min_samples_split': 91}
'Best performance: 0.815'
Grid search completed in 529.3 seconds

## Learning Curve

With the best model parameters, compute the learning curve.
Learning curve completed in 7.1 seconds
* Analysis: Clearly there is a trend which shows in general more data leads to higher performance. A dataset
    several times the one used would likely produce better results. Performance further improves when using 94%
    of the data, likely indicating there are a few harder examples that happen to get removed until more  than 94%
    of the data is used at the random seed used.

#############
Tree Experiment 2:
#############


With a larger dataset I will have to reduce the parameter search space.
Could any insights from experiment 1 help?

## Validation Curve

* Starting from default parameters, performed validation curve analysis on the following parameters:
    * ccp_alpha
        * Best test auc: .80 train auc .858
        * best param value 0.006
    * max-depth
        * best test auc: .792 train auc: .875
        * best param value 4
    * min_samples_leaf
        * best test auc: .806 train auc: .864
        * best param value 26
    * min_samples_split
        * best test auc: .789 train auc: .829
        * best param value 127
Notes: the shapes of the validation curves are very similar to ds1 and the optimal values are similar or the same for each parameter.
    * noteable difference is max-depth is higher for the dataset that contains more and less-specific features

Validation curve completed in 38.9 seconds

## Grid Search

Round 1
{'ccp_alpha': 0.0,
 'max_depth': 8,
 'min_samples_leaf': 36,
 'min_samples_split': 177}
'Best performance: 0.813'
Grid search completed in 837.1 seconds
(gatech) chrisfarr@Chris-Farr-MacBook-Pro assignment1 % 
Round 2
{'ccp_alpha': 0.0,
 'max_depth': 8,
 'min_samples_leaf': 44,
 'min_samples_split': 178}
'Best performance: 0.813'
Grid search completed in 535.6 seconds

* after multiple experiments found no improvements to the 3rd digit
* seeing very similar top performance in the small-ds2 compared to ds1
* The additional features in exchange for lower granularity gives no improvement
* When considering the small ds2 is self-reported vs clinically captured it performs
    very similarly. However this may be because the increased number of features helps, the
    evidence of this is the larger max-depth required for the small ds2 indicating more features
    are leveraged

## Learning Curve

Learning curve completed in 39.0 seconds
* Again the optimal performance is reached prior to 100% of data, but the trend
    that more data is better is clear.

###############
Experiment 3
#############

# Version idea 1: Use the optimal parameters on the small ds2 and perform validation curve
 and learning rate analysis directly (no grid-searching the large dataset)

# Version idea 2: Repeat full experiment on large dataset but with reduced number of parameters
    searched

# Hybrid idea: Do version 1, then pick 2 features to search based on results

## Generating validation curves
(hybrid idea)
Validation curve completed in 137.0 seconds

Optimal Params from experiment 2
    {'ccp_alpha': 0.0,
    'max_depth': 8,
    'min_samples_leaf': 44,
    'min_samples_split': 178}

* Starting from optimal parameters discovered in Experiment 2
    * ccp_alpha
        * Best test auc: .813 train auc .822
        * best param value 0.0
    * max-depth
        * best test auc: .813 train auc: .822
        * best param value 8
    * min_samples_leaf
        * best test auc: .814 train auc: .82
        * best param value 326
    * min_samples_split
        * best test auc: .821 train auc: .813
        * best param value 477

* Aside from min-samples-leaf and min-samples-split (which highly depend on the number of examples)
   the ccp-alph and max-depth alpha params are optimal
* min-samples-leaf and min-samples-split, although larger numbers are optimal there is very little
  impact to training or testing error from extreme small to extreme large values of the parameters
  indicating little impact to the model performance.
* max-depth has the most simple relationship with overfitting as the larger the depth
  the more likely it is to overfit. And when the value is small there appears to be zero
  overfitting.