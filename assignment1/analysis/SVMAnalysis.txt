### SVM Analysis

Preprocessing
Features were min-max scaled. L2 normalization was also tested, however the performance suffered. It was 
especially poor with ds2-small and ds2, perhaps this is because missing values are handled with 0 in many
of the features and this skews their distributions.

# Data preprocessing
* I tested min-max scaling and l2 normalizing as preprocessing functions for
    the input data and min-max scaling performed significantly better. I will use
    scaling for the remaining experiments.

validation curve parameters for experiments 1, 2 and 3
evaluation:
  validation_curve:
    # run for defaults
    kernel:
      start: 0
      stop: 4
      step: 1
    # run for poly kernel
    C:
      start: 0.001
      stop: 1.01
      step: .01
    coef0:
      start: 0.0
      stop: 1.01
      step: .05
    degree:
      start: 1
      stop: 7
      step: 1




############
Problem 1
#############

## Perform validation curve analysis

Generating validation curve...
Validation curve completed in 198.4 seconds

First step is to select which kernels might perform best by using only default parameters.
Top performers were:
    * rbf with test AUC .849 (train .887)
    * poly with an AUC less than rbf and greater than linear, also >.8, and train AUC > rbf. This tells me that it is fitting the data really well
        and there might be potential for higher performance than rbf when tuned.
    * then linear with and AUC less than poly but also > .8. This kernel also had the least amount of overfitting with the train score very close
        to the test score visually.
Due to the close to optimal test performance of poly as well as the optimal train performance, I decided to explore this kernel further.

set kernel to poly for remaining experiments

* C:
    * best test .849, train .882
    * param=0.141
* coef0:
    * best test .844, train .898
    * param=0.05
* degree:
    * best test .851, train .879
    * param=2

## Perform grid search

Round 1
Best parameters
{'model__C': 0.9510000000000001,
 'model__coef0': 0.15000000000000002,
 'model__degree': 2}
'Best performance: 0.849'
Grid search completed in 175.6 seconds
Round 2
Best parameters
{'model__C': 0.9700000000000002,
 'model__coef0': 0.17500000000000002,
 'model__degree': 2}
'Best performance: 0.849'
Grid search completed in 162.5 seconds
Round 3
Best parameters
{'model__C': 0.9, 'model__coef0': 0.19999999999999996, 'model__degree': 2}
'Best performance: 0.849'
Grid search completed in 117.2 seconds

## Compute learning curve
Generating learning curve...
Learning curve completed in 157.2 seconds

A blip at 17% reached peak performance of 86.9 test AUC. However, the general trend of using more data
is an increase in test score and a decrease in overfitting.


############
Problem 2
############

Runtime taking much longer.

With a max-iter of 10, testing 4 kernel types took 12.1 seconds
* best performance was test-auc=.588 and only rbf was > .5
With a max-iter of 100, 35 seconds
* best performance was test-auc=.552 and only linear was > .5
With a max-iter of 1000, 250 seconds
* best performance was test-auc=.506 and only linear was > .5

Apparantly the SVM has a quadratic runtime wrt the number of examples. 
One solution to this would be to produce an ensemble of SVC's and 
train each on a different subset of the data. 
https://stackoverflow.com/questions/31681373/making-svm-run-faster-in-python

Using 1000 estimators, each with 1/1000 of the dataset and no repeats
Poly runtime
Generating validation curve...
Validation curve completed in 211.3 seconds
* test auc: .803
Because I am only using this architecture for computing speed, I will
not fine-tune this 1000 estimators and will use this value for 
the remaining exercises of experiment 3.

Running linear
Generating validation curve...
Validation curve completed in 212.2 seconds

Runtime as a function of n-bagging-estimators
2000: 298
1000: 212
500: 177
*100: 163*
50: 176

Also reduced the k-folds from 5 to 3
Now the linear kernel is fitting in 100 seconds and this is manageable.

Generating validation curve...
Validation curve completed in 743.6 seconds

* linear
    * had the best test auc: .815 and train auc: .815
* poly and rbf had similar performance to linear
* simgoid was substantially lower performing

Near identical runtimes and performance. Will move forward with poly
for consistency.

Generating validation curve...
Validation curve completed in 1254.7 seconds

## Perform gridsearch


Round 1
Performing grid search...
Grid-Search Parameters:
{'model__base_estimator__C': [0.5, 1.0],
 'model__base_estimator__coef0': [0.5, 1.0],
 'model__base_estimator__degree': [2, 3]}
Fitting on 202944 train examples...
Best parameters
{'model__base_estimator__C': 1.0,
 'model__base_estimator__coef0': 1.0,
 'model__base_estimator__degree': 2}
'Best performance: 0.817'
Grid search completed in 253.8 seconds
Round 2
Performing grid search...
Grid-Search Parameters:
{'model__base_estimator__C': [1.5, 2.5, 3.5, 4.5],
 'model__base_estimator__coef0': [1.5, 2.5, 3.5, 4.5],
 'model__base_estimator__degree': [2]}
Fitting on 202944 train examples...
Best parameters
{'model__base_estimator__C': 3.5,
 'model__base_estimator__coef0': 4.5,
 'model__base_estimator__degree': 2}
'Best performance: 0.819'
Grid search completed in 514.9 seconds

## Generate the learning curve
Generating learning curve...
Learning curve completed in 2908.7 seconds

I see diminishing returns, however there is a steady improvement 
more training examples are used. The best performance is when using the
entire dataset.
This isn't what we saw when using the smaller datasets, however, this
is likely because the smaller datasets learners are easily influenced
based a small change in the number of examples.