Decision tree

## Preprocessing
* no preprocessing was utilized for all of the decision tree experiments. This is because 
I believe that decision trees are fairly robust to irregular scales and distributions since
the decision function only depends on a point within the disribution of the feature.
    

validation curve parameters for experiments 1, 2 and 3
ccp_alpha:
    start: 0.0
    stop: 0.051
    step: 0.001
min_samples_leaf:
    start: 1
    stop: 500
    step: 25

###########
Problem 1
###########

Additionally I compared the performance of gini vs entropy for the validation curves.
The performance was very similar although in some cases entropy had a test auc about .002
higher than gini. The runtime was also slightly longer, as i expected based on some research
about runtimes comparing the two (https://quantdare.com/decision-trees-gini-vs-entropy/)
I decided to stick with gini to have consistent default parameters throughout the three tree
experiments since the ds2 is significantly larger than ds1 and this could hurt the runtime for
the experiments.

## Grid-Search

grid-search parameters
* Performed grid-search cv from defaults and through max-performing values discovered above


Round 1
Performing grid search...
Grid-Search Parameters:
{'model__ccp_alpha': [0.0, 0.005, 0.01, 0.015],
 'model__min_samples_leaf': [15, 20, 25, 30, 35, 40, 45]}
Fitting on 614 train examples...
Best parameters
{'model__ccp_alpha': 0.0, 'model__min_samples_leaf': 30}
'Best performance: 0.811'
Grid search completed in 5.9 seconds

Performing cross-validation...
{'roc_auc_test': 0.8117447259136212, 'roc_auc_train': 0.8724288391005371}
Cross-validation completed in 3.9 seconds



## Learning Curve

With the best model parameters, compute the learning curve.
Learning curve completed in 7.1 seconds
* Analysis: Clearly there is a trend which shows in general more data leads to higher performance. A dataset
    several times the one used would likely produce better results. Performance further improves when using 94%
    of the data, likely indicating there are a few harder examples that happen to get removed until more  than 94%
    of the data is used at the random seed used.

Performing full test evaluation...
Training time: 0.002
Inference time: 0.000
Final Performance Evaluation:
{'test_auc': 0.7884259259259259, 'train_auc': 0.8797371495327103}

###############
Problem 2
#############

# Grid search

Round 1
Performing grid search...
Grid-Search Parameters:
{'model__ccp_alpha': [0.0,
                      0.001,
                      0.002,
                      0.003,
                      0.004,
                      0.005,
                      0.006,
                      0.007,
                      0.008,
                      0.009000000000000001,
                      0.01],
 'model__min_samples_leaf': [15, 65, 115, 165, 215, 265, 315, 365]}
Fitting on 202944 train examples...
Best parameters
{'model__ccp_alpha': 0.0, 'model__min_samples_leaf': 365}
'Best performance: 0.813'
Grid search completed in 33.2 seconds
Round 2
Performing grid search...
Grid-Search Parameters:
{'model__ccp_alpha': [0.0,
                      0.0005,
                      0.001,
                      0.0015,
                      0.002,
                      0.0025,
                      0.003,
                      0.0035,
                      0.004,
                      0.0045000000000000005],
 'model__min_samples_leaf': [300, 325, 350, 375, 400]}
Fitting on 202944 train examples...
Best parameters
{'model__ccp_alpha': 0.0, 'model__min_samples_leaf': 350}
'Best performance: 0.813'
Grid search completed in 17.1 seconds

Performing cross-validation...
{'roc_auc_test': 0.8140105797878864, 'roc_auc_train': 0.8219327453658414}
Cross-validation completed in 3.8 seconds


Generating learning curve...
Learning curve completed in 90.2 seconds

Performing full test evaluation...
Training time: 0.329
Inference time: 0.006
Final Performance Evaluation:
{'test_auc': 0.810099643329232, 'train_auc': 0.8227382128349737}