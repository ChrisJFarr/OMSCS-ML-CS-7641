Decision tree

## Preprocessing
* no preprocessing was utilized for all of the decision tree experiments. This is because 
I believe that decision trees are fairly robust to irregular scales and distributions since
the decision function only depends on a point within the disribution of the feature.
    

validation curve parameters for experiments 1, 2 and 3
max_depth:
    start: 2
    stop: 10
    step: 1
ccp_alpha:
    start: 0.0
    stop: 0.051
    step: 0.001
min_samples_split:
    start: 2
    stop: 500
    step: 25
min_samples_leaf:
    start: 1
    stop: 500
    step: 25

###########
Problem 1
###########

## Perform Validation Curve analysis

Validation curve completed in 34.3 seconds


* Starting from default parameters, performed validation curve analysis on the following parameters:
    * Top performance is test AUC=.81
        * achieved with all defaults and setting min_samples_leaf to 26
            * train AUC .879 test AUC=.812
        * achieved with all defaults and setting min_samples_split to 102
            * train AUC .862 test AUC=.810 (slightly less overfitting) 
    * max-depth
        * train .854 and test .793 with max-depth=3
        * this reduced overfitting at cost of some performance compared to top metrics
    * ccp_alpha
        * train .848 and test .794 with ccp_alpha=0.01
        * similar effect of max-depth on test performance (when viewing the curve) but inverse
            impact on the training


Additionally I compared the performance of gini vs entropy for the validation curves.
The performance was very similar although in some cases entropy had a test auc about .002
higher than gini. The runtime was also slightly longer, as i expected based on some research
about runtimes comparing the two (https://quantdare.com/decision-trees-gini-vs-entropy/)
I decided to stick with gini to have consistent default parameters throughout the three tree
experiments since the ds2 is significantly larger than ds1 and this could hurt the runtime for
the experiments.

## Grid-Search

grid-search parameters
* Performed grid-search cv from defaults and through max-performing values discovered above


Best parameters
Round 1 of grid-search
{'ccp_alpha': 0.0,
 'max_depth': 5,
 'min_samples_leaf': 16,
 'min_samples_split': 92}
'Best performance: 0.814'
Grid search completed in 284.4 seconds
Round 2 of grid-search
{'ccp_alpha': 0.0,
 'max_depth': 6,
 'min_samples_leaf': 20,
 'min_samples_split': 90}
'Best performance: 0.814'
Grid search completed in 120.1 seconds
Round 3 of grid-search
Best parameters
{'ccp_alpha': 0.0004,
 'max_depth': 6,
 'min_samples_leaf': 17,
 'min_samples_split': 91}
'Best performance: 0.815'
Grid search completed in 529.3 seconds

## Learning Curve

With the best model parameters, compute the learning curve.
Learning curve completed in 7.1 seconds
* Analysis: Clearly there is a trend which shows in general more data leads to higher performance. A dataset
    several times the one used would likely produce better results. Performance further improves when using 94%
    of the data, likely indicating there are a few harder examples that happen to get removed until more  than 94%
    of the data is used at the random seed used.


###############
Problem 2
#############

# Version idea 1: Use the optimal parameters on the small ds2 and perform validation curve
 and learning rate analysis directly (no grid-searching the large dataset)

# Version idea 2: Repeat full experiment on large dataset but with reduced number of parameters
    searched

# Hybrid idea: Do version 1, then pick 2 features to search based on results

## Generating validation curves
(hybrid idea)
Validation curve completed in 137.0 seconds

Optimal Params from experiment 2
    {'ccp_alpha': 0.0,
    'max_depth': 8,
    'min_samples_leaf': 44,
    'min_samples_split': 178}

* Starting from optimal parameters discovered in Experiment 2
    * ccp_alpha
        * Best test auc: .813 train auc .822
        * best param value 0.0
    * max-depth
        * best test auc: .813 train auc: .822
        * best param value 8
    * min_samples_leaf
        * best test auc: .814 train auc: .82
        * best param value 326
    * min_samples_split
        * best test auc: .821 train auc: .813
        * best param value 477

* Aside from min-samples-leaf and min-samples-split (which highly depend on the number of examples)
   the ccp-alph and max-depth alpha params are optimal
* min-samples-leaf and min-samples-split, although larger numbers are optimal there is very little
  impact to training or testing error from extreme small to extreme large values of the parameters
  indicating little impact to the model performance.
* max-depth has the most simple relationship with overfitting as the larger the depth
  the more likely it is to overfit. And when the value is small there appears to be zero
  overfitting.